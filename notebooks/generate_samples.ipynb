{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import nibabel as nb\n",
    "from nilearn.regions import RegionExtractor\n",
    "from skimage.feature.peak import peak_local_max\n",
    "from nilearn.image import smooth_img, new_img_like\n",
    "import numpy as np\n",
    "import os\n",
    "from scipy.misc import imsave\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "def np_to_tfrecords(X, Y, file_path_prefix, verbose=True):\n",
    "    \"\"\"\n",
    "    Converts a Numpy array (or two Numpy arrays) into a tfrecord file.\n",
    "    For supervised learning, feed training inputs to X and training labels to Y.\n",
    "    For unsupervised learning, only feed training inputs to X, and feed None to Y.\n",
    "    The length of the first dimensions of X and Y should be the number of samples.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : numpy.ndarray of rank 2\n",
    "        Numpy array for training inputs. Its dtype should be float32, float64, or int64.\n",
    "        If X has a higher rank, it should be rshape before fed to this function.\n",
    "    Y : numpy.ndarray of rank 2 or None\n",
    "        Numpy array for training labels. Its dtype should be float32, float64, or int64.\n",
    "        None if there is no label array.\n",
    "    file_path_prefix : str\n",
    "        The path and name of the resulting tfrecord file to be generated, without '.tfrecords'\n",
    "    verbose : bool\n",
    "        If true, progress is reported.\n",
    "    \n",
    "    Raises\n",
    "    ------\n",
    "    ValueError\n",
    "        If input type is not float (64 or 32) or int.\n",
    "    \n",
    "    \"\"\"\n",
    "    def _dtype_feature(ndarray):\n",
    "        \"\"\"match appropriate tf.train.Feature class with dtype of ndarray. \"\"\"\n",
    "        assert isinstance(ndarray, np.ndarray)\n",
    "        dtype_ = ndarray.dtype\n",
    "        if dtype_ == np.float64 or dtype_ == np.float32:\n",
    "            return lambda array: tf.train.Feature(float_list=tf.train.FloatList(value=array))\n",
    "        elif dtype_ == np.int64:\n",
    "            return lambda array: tf.train.Feature(int64_list=tf.train.Int64List(value=array))\n",
    "        else:  \n",
    "            raise ValueError(\"The input should be numpy ndarray. \\\n",
    "                               Instaed got {}\".format(ndarray.dtype))\n",
    "            \n",
    "    assert isinstance(X, np.ndarray)\n",
    "    assert len(X.shape) == 2  # If X has a higher rank, \n",
    "                               # it should be rshape before fed to this function.\n",
    "    assert isinstance(Y, np.ndarray) or Y is None\n",
    "    \n",
    "    # load appropriate tf.train.Feature class depending on dtype\n",
    "    dtype_feature_x = _dtype_feature(X)\n",
    "    if Y is not None:\n",
    "        assert X.shape[0] == Y.shape[0]\n",
    "        assert len(Y.shape) == 2\n",
    "        dtype_feature_y = _dtype_feature(Y)            \n",
    "    \n",
    "    # Generate tfrecord writer\n",
    "    result_tf_file = file_path_prefix + '.tfrecords'\n",
    "    writer = tf.python_io.TFRecordWriter(result_tf_file)\n",
    "    if verbose:\n",
    "        print(\"Serializing {:d} examples into {}\".format(X.shape[0], result_tf_file))\n",
    "        \n",
    "    # iterate over each sample,\n",
    "    # and serialize it as ProtoBuf.\n",
    "    for idx in range(X.shape[0]):\n",
    "        x = X[idx]\n",
    "        if Y is not None:\n",
    "            y = Y[idx]\n",
    "        \n",
    "        d_feature = {}\n",
    "        d_feature['X'] = dtype_feature_x(x)\n",
    "        if Y is not None:\n",
    "            d_feature['Y'] = dtype_feature_y(y)\n",
    "            \n",
    "        features = tf.train.Features(feature=d_feature)\n",
    "        example = tf.train.Example(features=features)\n",
    "        serialized = example.SerializeToString()\n",
    "        writer.write(serialized)\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"Writing {} done!\".format(result_tf_file))\n",
    "\n",
    "        \n",
    "class Map(object):\n",
    "    \n",
    "    def __init__(self, filename, min_distance=10, max_peaks=20):\n",
    "        \n",
    "        self.filename = filename\n",
    "        self.img = nb.load(filename)\n",
    "        self.peak_sets = []\n",
    "        self.peaks = peak_local_max(self.img.get_data(), exclude_border=False,\n",
    "                                    min_distance=min_distance,\n",
    "                                    num_peaks=max_peaks)\n",
    "\n",
    "    def distort_peaks(self, num_copies=1, max_peaks=None, jitter=None,\n",
    "                      include_orig=True):\n",
    "        \n",
    "        self.peak_sets = []\n",
    "        \n",
    "        if include_orig:\n",
    "            self.peak_sets.append(self.peaks)\n",
    "        \n",
    "        for i in range(num_copies):\n",
    "            \n",
    "            peaks = self.peaks.copy()\n",
    "            \n",
    "            if jitter is not None:\n",
    "                peaks += np.random.randint(0, high=jitter+1, size=peaks.shape)\n",
    "                peaks = np.clip(peaks, [0, 0, 0], [90, 108, 90])\n",
    "\n",
    "            if max_peaks is not None and len(peaks) > max_peaks:\n",
    "                np.random.shuffle(peaks)\n",
    "                peaks = peaks[:max_peaks, :]\n",
    "            \n",
    "            self.peak_sets.append(peaks)\n",
    "        \n",
    "    def to_tfrecords(self, output_dir, max_peaks=5, max_slices=3):\n",
    "        ''' Write peak sets and slices from the current map out as TFRecords. '''\n",
    "\n",
    "        basename = os.path.basename(self.filename).split('.')[0]\n",
    "\n",
    "        for i, ps in enumerate(self.peak_sets):\n",
    "\n",
    "            # Determine number of slices\n",
    "            avail_slices = np.unique(ps[:, 2])\n",
    "            np.random.shuffle(avail_slices)\n",
    "            n_slices = min(max_slices, len(avail_slices))\n",
    "\n",
    "            for j in range(n_slices):\n",
    "                sl_ind = avail_slices[j]\n",
    "                X_coords = ps[ps[:,2]==j]\n",
    "                if max_peaks is not None and len(X_coords) > max_peaks:\n",
    "                    np.random.shuffle(X_coords)\n",
    "                    X_coords = X_coords[:max_peaks]\n",
    "                \n",
    "                # Make image slices\n",
    "                X_img = np.zeros(self.img.shape[:2])\n",
    "                X_img[tuple(X_coords[:,:2].T)] = 1\n",
    "                Y_img = self.img.get_data()[:, :, sl_ind]\n",
    "                \n",
    "                # Write to TFRecords\n",
    "                map_suff = '_var-%d_zslice-%d' % ((i+1), sl_ind)\n",
    "                path = os.path.join(output_dir, basename + map_suff)\n",
    "                np_to_tfrecords(X_img, Y_img, path, verbose=False)\n",
    "\n",
    "                \n",
    "def volumes_to_tfrecords(images, output_path, min_distance=10,\n",
    "                       max_peaks_per_volume=100, max_peaks_per_slice=5,\n",
    "                       max_slices=3, num_copies=4, jitter=3,\n",
    "                       include_orig=True):\n",
    "    ''' Reads in a set of nifti volumes and outputs TFRecords objects for 2D\n",
    "    slices, optionally with some distortion/duplication.\n",
    "    Args:\n",
    "        images (str or list): Either a string giving the path to a set of nifti images,\n",
    "            or a list of already-loaded NiftiImages.\n",
    "        output_path (str): Directory to write tfrecords to.\n",
    "        min_distance (int): Minimum distance between peaks (in voxels)\n",
    "        max_peaks_per_volume (int): Maximum number of peaks to extract for each volume\n",
    "        max_peaks_per_slice (int): Maximum number of peaks to extract for each slice\n",
    "        max_slices (int): Maximum number of slices to extract from each volume\n",
    "        num_copies (int): Number of distorted copies of each slice to create\n",
    "        jitter (int): Amount of X/Y jittering of each peak (in voxels) to inject\n",
    "        include_orig (bool): if True, includes the original, unaltered slice in training set \n",
    "    '''\n",
    "    if isinstance(images, str):\n",
    "        images = glob(images)\n",
    "    maps = [Map(img, min_distance, max_peaks_per_volume) for img in images]\n",
    "    for m in maps:\n",
    "        if num_copies:\n",
    "            m.distort_peaks(num_copies, max_peaks_per_volume, jitter=jitter,\n",
    "                            include_orig=include_orig)\n",
    "        m.to_tfrecords(output_path, max_peaks_per_slice, max_slices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# E.g...\n",
    "images = glob('/Dropbox/files/HCP/original/*.nii.gz')\n",
    "volumes_to_tfrecords(images[:10], '/mnt/c/Users/tyark/Downloads/tmp')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
